---
title: "Model 3"
author: "SITTY AZQUIA M. CAMAMA"
date: "2022-12-15"
output:
  pdf_document: default
  html_document: default
---

# Clustering Techniques

In this document, we will perform and compare the following clustering techniques results such as K-Means, Hierarchical and Model based clustering without considering the binary output and categorical variables in the data. In these models, **radiomics data** is utilized.

## 1. K-Means Clustering

K-Means Clustering is one of the most well-known and commonly used clustering algorithms for partitioning observations into a set of k groups. 

### Load Helper Packages
```{r}
library(dplyr)       # for data manipulation
library(ggplot2)     # for data visualization
library(stringr)     # for string functionality
library(gridExtra)   # for manipulating the grid
library(bestNormalize)
```

### Load Modeling Packages
```{r}
library(tidyverse)   # data manipulation
library(cluster)     # for general clustering algorithms
library(factoextra)  # for visualizing cluster results
library(mclust)      # for fitting clustering algorithms
```

### Load Data Sets

Radiomics data contains 197 rows and 431 columns: 
**Failure.binary**: binary property to predict

```{r}
radiomicsdata <- read.csv("~/R CLASS/FINAL PROJECT/radiomics_completedata.csv")
View(radiomicsdata)
```

## Data Pre-Processing

### Check for null and missing values

Using **anyNA()** function, We can determine if any missing values in our data. The result shows either **TRUE** or **FALSE**. If true, omit the missing values using **na.omit().** Hence, our data has no missing values.

```{r}
anyNA(radiomicsdata)
```

### Check for normality

The **Shapiro-Wilk's Test** is used to check the normality of the data. The null hypothesis states that data are normally distributed. Before, we test the normality, remove the categorical and binary variable.

```{r,warning=F}
rd <- radiomicsdata%>%select_if(is.numeric) 
rd <- rd[,-1]
test <- apply(rd,2,function(x){shapiro.test(x)})
```

unlist() function is used to convert a list to vector, so we can have the list of p-value of all variables.

```{r}
pvalue_list <- unlist(lapply(test, function(x) x$p.value))
```

Compute the sum of total variable with p-value less than 0.05 alpha. Thus, we have 428 variables that are not normally distributed and Entropy_cooc.W.ADC is normally distributed.

```{r}
sum(pvalue_list<0.05)  # not normally distributed
sum(pvalue_list>0.05)  # normally distributed
test$Entropy_cooc.W.ADC
```

To normalized the data, remove first the categorical, binary and Entropy_cooc.W.ADC variable and use **orderNorm()** function. The **x.t**	is the elements of orderNorm() function transformed original data.

```{r,warning=F}
rdnorm=radiomicsdata[,c(3,5:length(names(radiomicsdata)))]
rdnorm=apply(rdnorm,2,orderNorm)
rdnorm=lapply(rdnorm, function(x) x$x.t)
rdnorm=rdnorm%>%as.data.frame()
```

Test again using shapiro-wilk's test.

```{r,warning=F}
test2=apply(rdnorm,2,shapiro.test)
pvalue_list2=unlist(lapply(test2, function(x) x$p.value))
```

Compute the sum of total variable with p-value less than 0.05 alpha and more than 0.05 alpha. Finally, our data is normally distributed.

```{r,warning=F}
sum(pvalue_list2<0.05)   # not normally distributed
sum(pvalue_list2>0.05)   # normally distributed
```

Create new data with the **Entropy_cooc.W.ADC**, and **rdnorm** variables. 

```{r,warning=F}
keep = select(radiomicsdata, c("Entropy_cooc.W.ADC"))
df = cbind(keep,rdnorm)
View(df)
```

### Apply K-Means Clustering Algorithm

The main goal of k-means clustering is to **create clusters** with a total within-cluster variation that is minimized. So, perform K-means clustering with 3 clusters, 100 maximum number of iterations, and 100 nstart. 

Let's start at 2 clusters of sizes 144, 50 have Within cluster sum of squares of 42657.82, 13404.39, respectively. 

```{r}
k <-kmeans(df, centers = 2, iter.max = 100, nstart = 100)
k
```

The quality of the k-means partition is measured by the **SSwithin**, and we want it to be as little as feasible. Thus, we have 33.2%.

```{r}
k$betweenss/k$totss
```

To plot the 2 clusters, use **fviz_cluster()** function.

```{r}
fviz_cluster(k, data = df)
```

### Determining Optimal Clusters

Using **Within Sum of Squares**, **Silhouette** and **gap_stat** plots, are another method to determine the optimal value of K number of clusters. It suggest with 2 clusters.

```{r}
fviz_nbclust(df, kmeans, method = "wss") 
fviz_nbclust(df, kmeans, method = "silhouette")
fviz_nbclust(df, kmeans, method = "gap_stat") 
```

Visualize clusters using the original variables where **x is Failure** and **y is Entropy_cooc.W.ADC**

```{r}
radiomicsdata <- radiomicsdata |> mutate(cluster = k$cluster)
radiomicsdata |> ggplot(aes(x = Failure, y = Entropy_cooc.W.ADC, col = as.factor(cluster))) + geom_point()
```

## 2. Heirarchical Clustering

An alternate method to k-means clustering for identifying groupings in a data set is hierarchical clustering. Unlike kmeans,the number of clusters does not need to be predetermined because in this method will build a hierarchy of clusters.

### Standardize Data

Before building a clustering model, standardization of data is required.

```{r}
hdf <- radiomicsdata %>%
  select_if(is.numeric) %>%  # select numeric columns
  select(-Failure.binary) %>%    # remove target column
  mutate_all(as.double) %>%  # coerce to double type
  scale()
```

### Apply Heirarchical Clustering Algorithm

Similar to k-means, we compute first the dissimilarity of observations using distance measures to get the agglomerative coefficient (AC). Using **hclust() function**, we can feed these values and specify the agglomeration method to be used either **"complete", "average", "single", or "ward.D2"**

```{r}
#Dissimilarity matrix
d <- dist(hdf, method = "euclidean")

## Hierarchical clustering using Complete Linkage
h1 <- hclust(d, method = "complete")
sub_grp1 <- cutree(h1, k = 8) # Cut tree into 8 groups
table(sub_grp1)  # Number of members in each cluster
plot(h1, cex=0.7)

# Using Ward's method
h2 <- hclust(d, method = "ward.D2" )
sub_grp <- cutree(h2, k = 8)
table(sub_grp)
plot(h2, cex=0.7)
```
### Using Agglomerative  Hierarchical Clustering

We can also use the agnes() function as alternative way to get the agglomerative coefficient (AC), which measures the amount of clustering structure found.

```{r}
set.seed(123)
h3 <- agnes(hdf, method = "complete")

#agglomerative coefficient
h3$ac

# another way to compute coefficient
ac <- function(x) {
  agnes(hdf, method = x)$ac
}

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```
### Using Divisive Hierarchical Clustering

Aside from agglomeration method, we can also perform divisive hierarchical clustering which **diana() function** allows us to perform. However, there is no agglomerative coefficient to give but divisive coefficient (DC).

```{r}
h4 <- diana(hdf)

#Divisive coefficient
h4$dc
```
### Determining Optimal Clusters

Determining optimal clusters using **Elbow method**, **Silhouette** and **gap_stat** plots. It reveals that in elbow method and silhoutte suggest 2 clusters while 9 clusters in gap statistic.

```{r}
p1 <- fviz_nbclust(hdf, FUN = hcut, method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")
p2 <- fviz_nbclust(hdf, FUN = hcut, method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
p3 <- fviz_nbclust(hdf, FUN = hcut, method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)

```

## 3. Model-Based Clustering

The advantage of model-based clustering over K-means and hierarchical clustering is that it automatically determines the ideal number of clusters. In this clustering, Gaussian mixture models is applied, which are one of the most popular model-based clustering approaches available. Using **df** values in k-means clustering since it is already standardized, we can use **Mclust() function**. Leaving **G = NULL** forces Mclust() to evaluate 1–9 clusters and select the optimal number of components based on BIC. 

```{r}
mb <- Mclust(df[,1:10], G=NULL) 
summary(mb)
```

The result shows 3 optimal number of clusters with BIC -2632.206. A negative zone with the highest value indicates the preferred model, In general, the lower the BIC value, the better. Plot the results with BIC, density and uncertainty.

```{r}
legend_args <- list(x = "bottomright", ncol = 5)
plot(mb, what = 'BIC', legendArgs = legend_args)
plot(mb, what = "density")
plot(mb, what = "uncertainty")
```

Plot the distribution of probabilities for all observations aligning to each of the 3 clusters. As clusters have more observations with middling levels of probability (i.e., 0.25–0.75), their clusters are usually less compact. Therefore, C3 is less compact than other clusters.

```{r}
probabilities <- mb$z 
colnames(probabilities) <- paste0('C', 1:3)

probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)

ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)
```

Plot the observations that are aligned to each cluster but their uncertainty of membership is greater than 0.25.

```{r}
uncertainty <- data.frame(
  id = 1:nrow(df),
  cluster = mb$classification,
  uncertainty = mb$uncertainty
)

uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.25) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)
```

Plot the average standardized consumption for cluster 2 observations compared to all observations.

```{r}
cluster2 <- df %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = mb$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)

cluster2 %>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average standardized consumption", y = NULL)
```
